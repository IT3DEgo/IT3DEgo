{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c8d93de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "import cv2\n",
    "import json\n",
    "import csv\n",
    "import shutil\n",
    "import PIL\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103e259a",
   "metadata": {},
   "source": [
    "## This notebook demonstrates the pipeline of the improved baseline, i.e., SAM+DINOv2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a31142",
   "metadata": {},
   "source": [
    "### step 1: Computing per-frame 2D detections with cosine simlarity  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cd1becf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def extract_features_dinov2(model, img_tensor, use_cuda=True):\n",
    "    samples = img_tensor.cuda(non_blocking=True)\n",
    "    feats = model(samples).float().clone()\n",
    "    return feats\n",
    "\n",
    "def extract_features(encoder_arch, encoder, visual_crop_np, preprocess, device):\n",
    "    with torch.no_grad():\n",
    "        if encoder_arch == \"DINOv2\":\n",
    "            input_list = []\n",
    "            for vc in visual_crop_np:\n",
    "                image_input = preprocess(PIL.Image.fromarray(vc)).unsqueeze(0).to(device)\n",
    "                input_list.append(image_input)\n",
    "            all_input = torch.cat(input_list, dim=0)\n",
    "            visual_crop_features = extract_features_dinov2(encoder, all_input) # N x 1024\n",
    "\n",
    "    return visual_crop_features\n",
    "\n",
    "def load_online_enrollment(annot_data_path, xywh2xyxy=False):\n",
    "    bbox = {}\n",
    "    frame = {}\n",
    "    with open(os.path.join(annot_data_path, \"svoe.txt\"), \"r\") as f:\n",
    "        for l in f:\n",
    "            l_list = l.rstrip(\"\\n\").split()\n",
    "            temp = np.array(l_list[2:]).astype(int)\n",
    "            if xywh2xyxy:\n",
    "                temp[2:] += temp[0:2]\n",
    "            bbox[l_list[0]] = temp\n",
    "            frame[l_list[0]] = l_list[1]\n",
    "    return bbox, frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1129af56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_frame_proposals(detection_f, detector=\"sam\", unnormalize=False, img_W=None, img_H=None, include_conf=True):\n",
    "    res = [] # XYHW format\n",
    "    if detector == \"sam\":\n",
    "        with open(detection_f, \"r\") as f:\n",
    "            for l in f:\n",
    "                l = l.strip().split()\n",
    "                if len(l) == 0:\n",
    "                    continue\n",
    "                x, y, w, h = float(l[0]), float(l[1]), float(l[2]), float(l[3])\n",
    "                if len(l) > 4 and include_conf:\n",
    "                    conf = float(l[4])\n",
    "                xyxy = np.array([x, y, x+w, y+h], dtype=np.float32)\n",
    "                if include_conf:\n",
    "                    res.append(np.array([xyxy[0], xyxy[1], xyxy[2], xyxy[3], conf], dtype=np.float32))\n",
    "                else:\n",
    "                    res.append(np.array([xyxy[0], xyxy[1], xyxy[2], xyxy[3]], dtype=np.float32))\n",
    "\n",
    "    return res\n",
    "\n",
    "def extract_windows_from_detections(frame, all_detections):\n",
    "    # add dets must in XYXY mode\n",
    "    res = []\n",
    "    for det in all_detections:\n",
    "        x1, y1, x2, y2 = det[:4]\n",
    "        window = frame[int(y1):int(y2), int(x1):int(x2)]\n",
    "        res.append(window)\n",
    "    return res\n",
    "\n",
    "def extract_proposal_scores(proposal_features, visual_crop_features, det_score=None, dist_metric=\"cos\", score_thresh=None, topk=1):\n",
    "    # unsqueeze to M x N x D\n",
    "    proposal_features = proposal_features.unsqueeze(0).expand(visual_crop_features.shape[0], -1, -1)\n",
    "    visual_crop_features = visual_crop_features.unsqueeze(1).expand(-1, proposal_features.shape[1], -1)\n",
    "\n",
    "    # compute proposal scores\n",
    "    if dist_metric == \"cos\":\n",
    "        proposal_scores = torch.nn.functional.cosine_similarity(proposal_features, visual_crop_features, dim=-1) # M x N\n",
    "\n",
    "    if score_thresh is not None:\n",
    "        proposal_scores[proposal_scores < score_thresh] = 0\n",
    "\n",
    "    # extract top k results\n",
    "    proposal_ret = torch.topk(proposal_scores, k=topk, dim=1)\n",
    "\n",
    "    return proposal_ret\n",
    "\n",
    "def load_instance_id_pairs(label_csv):\n",
    "    with open(label_csv, \"r\") as f:\n",
    "        data = list(csv.reader(f, delimiter=\",\"))\n",
    "    res = {}\n",
    "    for i, v in enumerate(data):\n",
    "        res[i] = v[0] # 0: \"shoe_1\"\n",
    "    return res\n",
    "\n",
    "def _convert_image_to_rgb(image):\n",
    "    return image.convert(\"RGB\")\n",
    "\n",
    "def print_msg_box(msg, indent=1, width=None, title=None):\n",
    "    \"\"\"Print message-box with optional title.\"\"\"\n",
    "    lines = msg.split('\\n')\n",
    "    space = \" \" * indent\n",
    "    if not width:\n",
    "        width = max(map(len, lines))\n",
    "    box = f'╔{\"═\" * (width + indent * 2)}╗\\n'  # upper_border\n",
    "    if title:\n",
    "        box += f'║{space}{title:<{width}}{space}║\\n'  # title\n",
    "        box += f'║{space}{\"-\" * len(title):<{width}}{space}║\\n'  # underscore\n",
    "    box += ''.join([f'║{space}{line:<{width}}{space}║\\n' for line in lines])\n",
    "    box += f'╚{\"═\" * (width + indent * 2)}╝'  # lower_border\n",
    "    print(box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e72a7758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╔════════════════════════════╗\n",
      "║ model: SVOE_sam_DINOv2_0.6 ║\n",
      "╚════════════════════════════╝\n",
      "Step 1 done!\n"
     ]
    }
   ],
   "source": [
    "data_root = os.path.join(os.getcwd(), \"../benchmark_data\")\n",
    "detection_root = os.path.join(os.getcwd(), \"../hololens_detection\")\n",
    "seq_list = os.listdir(os.path.join(os.getcwd(), \"../benchmark_data/raw_video_seqs\"))\n",
    "\n",
    "img_ext = \".png\"\n",
    "detector = \"sam\"\n",
    "enrollment_type = \"SVOE\"\n",
    "\n",
    "# create encoder\n",
    "encoder_arch = \"DINOv2\"\n",
    "score_thresh_dict = {\"DINOv2\": 0.6}\n",
    "model_name = \"{}_{}_{}_{}\".format(enrollment_type, detector, encoder_arch, score_thresh_dict[encoder_arch])\n",
    "\n",
    "print_msg_box(\"model: {}\".format(model_name))\n",
    "save_dir = os.path.join(os.getcwd(), \"results/association_results\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "cache_feature = True\n",
    "recompute_feature = False\n",
    "cache_dir = os.path.join(os.getcwd(), \"cache_features\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if encoder_arch == \"DINOv2\":\n",
    "    encoder = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14', verbose=False)\n",
    "    n_px = 256\n",
    "    crop_px = 224\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(n_px, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "        transforms.CenterCrop(crop_px),\n",
    "         _convert_image_to_rgb,\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ])\n",
    "    encoder.to(device)\n",
    "    encoder.eval()\n",
    "\n",
    "else:\n",
    "    raise RuntimeError(\"Unknown encoder architecture\")\n",
    "\n",
    "for seq in seq_list:\n",
    "    cam_pose = json.load(open(os.path.join(data_root, \"raw_video_seqs\", seq, \"pv_pose.json\")))\n",
    "    all_frames = sorted(cam_pose.keys())\n",
    "    save_dir_seq = os.path.join(save_dir, seq, model_name)\n",
    "    if os.path.exists(save_dir_seq):\n",
    "        continue\n",
    "    os.makedirs(save_dir_seq, exist_ok=True)\n",
    "\n",
    "    # check if feature is cached\n",
    "    obj_feature_fname = \"{}_{}\".format(enrollment_type, encoder_arch)\n",
    "    visual_crop_features_cache_p = os.path.join(cache_dir, seq, \"objects\", \n",
    "        \"{}.pth\".format(obj_feature_fname))\n",
    "\n",
    "    if os.path.exists(visual_crop_features_cache_p) and not recompute_feature:\n",
    "        print(\"load cached features!\")\n",
    "        visual_crop_features = torch.load(visual_crop_features_cache_p)\n",
    "    else:\n",
    "        if enrollment_type == \"SVOE\":\n",
    "            bbox_dict, frame_dict = load_online_enrollment(os.path.join(data_root, \"annotations\", seq.replace(\"raw\", \"annotation\")))\n",
    "            visual_crop_np = []\n",
    "            for obj_idx in frame_dict.keys():\n",
    "                f_p = os.path.join(data_root, \"raw_video_seqs\", seq, \"pv\", frame_dict[obj_idx] + img_ext)\n",
    "                f_img = cv2.cvtColor(cv2.imread(f_p), cv2.COLOR_BGR2RGB)\n",
    "                f_bbox = bbox_dict[obj_idx]\n",
    "                visual_crop_np += extract_windows_from_detections(f_img, [f_bbox])\n",
    "\n",
    "        visual_crop_features = extract_features(encoder_arch, encoder, visual_crop_np, preprocess, device)\n",
    "        if cache_feature:\n",
    "            os.makedirs(os.path.join(cache_dir, seq, \"objects\"), exist_ok=True)\n",
    "            torch.save(visual_crop_features.cpu(), os.path.join(cache_dir, seq, \"objects\", \"{}.pth\".format(obj_feature_fname)))\n",
    "\n",
    "    num_obj = visual_crop_features.shape[0]\n",
    "    count = 0\n",
    "    for f in all_frames:\n",
    "        frame_p = os.path.join(data_root, \"raw_video_seqs\", seq, \"pv\", f+img_ext)\n",
    "        frame = cv2.cvtColor(cv2.imread(frame_p), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # load detection\n",
    "        detection_f = os.path.join(detection_root, detector, seq, \"labels\", f+\".txt\")\n",
    "        if not os.path.exists(detection_f):\n",
    "            # still save an empty results\n",
    "            output_f = os.path.join(save_dir_seq, f+\".txt\")\n",
    "            with open(output_f, \"w\") as f:\n",
    "                pass\n",
    "            continue\n",
    "\n",
    "        all_detections = load_frame_proposals(detection_f, detector=detector, unnormalize=True, img_W=frame.shape[1], img_H=frame.shape[0])\n",
    "        proposal_features_cache_p = os.path.join(cache_dir, seq, \"proposals\", \"{}_{}_{}.pth\".format(f, detector, encoder_arch))\n",
    "\n",
    "        if os.path.exists(proposal_features_cache_p):\n",
    "            proposal_features = torch.load(proposal_features_cache_p)\n",
    "        else:\n",
    "            detection_windows = extract_windows_from_detections(frame, all_detections) # a list of windows: H x W x 3\n",
    "            proposal_features = extract_features(encoder_arch, encoder, detection_windows, preprocess, device)\n",
    "\n",
    "            if cache_feature:\n",
    "                # check whether feature exists\n",
    "                os.makedirs(os.path.join(cache_dir, seq, \"proposals\"), exist_ok=True)\n",
    "                cache_f_seq = \"{}_{}_{}.pth\".format(f, detector, encoder_arch)\n",
    "                torch.save(proposal_features.cpu(), os.path.join(cache_dir, seq, \"proposals\", cache_f_seq))\n",
    "\n",
    "        # extract features and find the closest detection proposal\n",
    "        det_scores = torch.tensor(np.array([det[4] for det in all_detections], dtype=np.float32))\n",
    "        proposal_ret = extract_proposal_scores(proposal_features.cpu().float(), visual_crop_features.cpu().float(), det_scores,\n",
    "                                               dist_metric=\"cos\", score_thresh=score_thresh_dict[encoder_arch], topk=1)\n",
    "        proposal_ret_val, proposal_ret_idx = proposal_ret.values.cpu().numpy(), proposal_ret.indices.cpu().numpy() # M x topk\n",
    "\n",
    "        '''\n",
    "            output format: visual_crop_id, associated detection #1, detection conf, similarity score\n",
    "        '''\n",
    "        output_f = os.path.join(save_dir_seq, f+\".txt\")\n",
    "        with open(output_f, \"w\") as f:\n",
    "            for i in range(num_obj):\n",
    "                for j in range(proposal_ret_idx.shape[-1]):\n",
    "                    if proposal_ret_val[i, j] == 0.: continue\n",
    "                    f.write(\"{} {} {} {} {} {} {}\\n\".format(i, *all_detections[proposal_ret_idx[i, j]], proposal_ret_val[i, j]))\n",
    "print(\"Step 1 done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5053d9",
   "metadata": {},
   "source": [
    "### step 2: Lifting the center of 2D bounding boxes to 3D  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f59ca722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "from proj_utils import compute_depth_scale_map_wrapper, pv_bbox2depth_unproj, to_homogeneous\n",
    "\n",
    "sys.path.insert(0, os.getcwd())\n",
    "import hl2ss_3dcv\n",
    "\n",
    "def singular_matrix(matrix):\n",
    "    return np.linalg.det(matrix) == 0\n",
    "\n",
    "def split_letter_number(s):\n",
    "    return re.split('(\\d+)',s)\n",
    "\n",
    "def load_2d_dets(annot_l, include_pred_score=False):\n",
    "    res = defaultdict(list) # key is frame number, values are boxes\n",
    "    for idx, annot_f in enumerate(annot_l):\n",
    "        frame_temp = os.path.basename(annot_f).split(\".\")[0]\n",
    "        with open(annot_f, \"r\") as f:\n",
    "            for l in f:\n",
    "                if len(l.split()) > 2:\n",
    "                    splits = l.rstrip(\"\\n\").split()\n",
    "                    obj_idx, x1, y1, x2, y2 = [int(float(xx)) for xx in splits[:5]]\n",
    "                    if int(x1) <= 1 and int(y1) <= 1:\n",
    "                        continue\n",
    "\n",
    "                    if include_pred_score and len(splits) == 6:\n",
    "                        if splits[-1].isnumeric():\n",
    "                            score = float(splits[-1])\n",
    "                        else:\n",
    "                            score = 1.0\n",
    "                        res[frame_temp].append(np.array([int(x1), int(y1), int(x2), int(y2), obj_idx, score], dtype=float))\n",
    "                    else:\n",
    "                        res[frame_temp].append(np.array([int(x1), int(y1), int(x2), int(y2), obj_idx], dtype=int))\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77b7affa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╔════════════════════════════╗\n",
      "║ model: SVOE_sam_DINOv2_0.6 ║\n",
      "╚════════════════════════════╝\n",
      "Step 2 done!\n"
     ]
    }
   ],
   "source": [
    "associate_root = os.path.join(os.getcwd(), \"results/association_results\")\n",
    "save_dir = os.path.join(os.getcwd(), \"results/lifted_3d_results\")\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "cam2world = True\n",
    "\n",
    "print_msg_box(\"model: {}\".format(model_name))\n",
    "ahat_calibration = hl2ss_3dcv._load_calibration_rm_depth_ahat(os.path.join(data_root, \"calibrations\", \"rm_depth_ahat\"))\n",
    "\n",
    "for seq in seq_list:\n",
    "    cam_pose = json.load(open(os.path.join(data_root, \"raw_video_seqs\", seq, \"pv_pose.json\")))\n",
    "    depth_poses = json.load(open(os.path.join(data_root, \"raw_video_seqs\", seq, \"depth_ahat_pose.json\")))\n",
    "    all_frames = sorted(cam_pose.keys(), key=lambda x:int(x))\n",
    "    save_dir_seq = os.path.join(save_dir, seq, model_name)\n",
    "    if os.path.exists(save_dir_seq):\n",
    "        continue\n",
    "    os.makedirs(save_dir_seq, exist_ok=True)\n",
    "\n",
    "    tracker_annot_dir = os.path.join(associate_root, seq, model_name, \"*.txt\")\n",
    "    tracker_annot_l = glob.glob(tracker_annot_dir)\n",
    "    tracker_annot_dict = load_2d_dets(tracker_annot_l, include_pred_score=True)\n",
    "\n",
    "    hw = {'image': (720, 1280), 'ahat_depth': (512, 512)}\n",
    "    count = 0\n",
    "    for f in all_frames:\n",
    "        frame_abs_path = os.path.join(data_root, \"raw_video_seqs\", seq, \"pv\", f + img_ext)\n",
    "        reproj_xyz = compute_depth_scale_map_wrapper(frame_abs_path, seq, hw=hw, depth_pose=depth_poses[f], rgb_pose=cam_pose[f])\n",
    "\n",
    "        assoc_dets = defaultdict(list)\n",
    "        scores = defaultdict(list)\n",
    "        if f not in tracker_annot_dict.keys():\n",
    "            det_flag = 0\n",
    "        else:\n",
    "            det_flag = 1\n",
    "            bbox_l = tracker_annot_dict[f] # [x1, y1, x2, y2, idx, (pred_score)], XYXY format already\n",
    "            for bbox in bbox_l:\n",
    "                    object_id = bbox[4]\n",
    "                    det = [float(x) for x in bbox[:4]]\n",
    "                    det_tmp, det_flag = pv_bbox2depth_unproj(det, reproj_xyz) # convert 2D detection into 3D pts\n",
    "                    if det_flag == 1:\n",
    "                        if cam2world and not singular_matrix(cam_pose[f]):\n",
    "                            matrix_t = hl2ss_3dcv.camera_to_rignode(ahat_calibration.extrinsics) @ hl2ss_3dcv.reference_to_world(depth_poses[f])\n",
    "                            det_tmp = to_homogeneous(det_tmp).reshape(1, 4) @ matrix_t\n",
    "                        assoc_dets[object_id].append(det_tmp.squeeze()[:3])\n",
    "                        if len(bbox) > 5:\n",
    "                            scores[object_id].append(bbox[-1])\n",
    "                        else:\n",
    "                            scores[object_id].append(1) # placeholder\n",
    "\n",
    "        # save results\n",
    "        '''\n",
    "            results format: visual_crop_id, associated 3d Point in world coord #1, detection conf, similarity score\n",
    "        '''\n",
    "        if det_flag == 1:\n",
    "            output_f = os.path.join(save_dir_seq, f+\".txt\")\n",
    "            with open(output_f, \"w\") as f2:\n",
    "                for i in assoc_dets.keys():\n",
    "                    for j in range(len(assoc_dets[i])):\n",
    "                        f2.write(\"{} {} {} {} {}\\n\".format(i, *assoc_dets[i][j], scores[i][j]))\n",
    "\n",
    "print(\"Step 2 done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a36fb0",
   "metadata": {},
   "source": [
    "### step 3: Adopting the simple memory mechanism for the frames without valid predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "231c331c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╔════════════════════════════╗\n",
      "║ model: SVOE_sam_DINOv2_0.6 ║\n",
      "╚════════════════════════════╝\n",
      "Step 3 done!\n"
     ]
    }
   ],
   "source": [
    "save_dir = os.path.join(os.getcwd(), \"results/baseline_results\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "print_msg_box(\"model: {}\".format(model_name))### step 3: Adopting the simple memory mechanism for the frames without valid predictions\n",
    "for seq in seq_list:\n",
    "    # load per-frame association results, camera poses, and object id\n",
    "    cam_pose = json.load(open(os.path.join(data_root, \"raw_video_seqs\", seq, \"pv_pose.json\")))\n",
    "    all_frames = sorted(cam_pose.keys(), key=lambda x:int(x))\n",
    "    save_dir_seq = os.path.join(save_dir, seq, model_name)\n",
    "    os.makedirs(save_dir_seq, exist_ok=True)\n",
    "\n",
    "    memory = {}\n",
    "    score_memory = {}\n",
    "    for f in all_frames:\n",
    "        # if 3d assocation results exist, load it == also update memory\n",
    "        if os.path.exists(os.path.join(associate_root, seq, model_name, f+\".txt\")):\n",
    "            with open(os.path.join(associate_root, seq, model_name, f+\".txt\"), \"r\") as f1:\n",
    "                lines = f1.readlines()\n",
    "                for line in lines:\n",
    "                    line = line.strip().split(\" \")\n",
    "                    loc_np = np.array(line[1:4], dtype=np.float32)\n",
    "                    scores = float(line[-1])\n",
    "\n",
    "                    if line[0] not in memory:\n",
    "                        memory[line[0]] = [loc_np]\n",
    "                        score_memory[line[0]] = [scores]\n",
    "                    elif (loc_np == memory[line[0]][-1]).all():\n",
    "                        continue\n",
    "                    else:\n",
    "                        memory[line[0]].append(loc_np)\n",
    "                        score_memory[line[0]].append(scores)\n",
    "\n",
    "        # output 3d association results,\n",
    "        with open(os.path.join(save_dir_seq, f+\".txt\"), \"w\") as f2:\n",
    "            for k, v in memory.items():\n",
    "                if len(v) >= 1:\n",
    "                    f2.write(\"{} {} {} {} {}\\n\".format(int(float(k)), *v[-1], score_memory[k][-1]))\n",
    "\n",
    "print(\"Step 3 done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ed8fd9",
   "metadata": {},
   "source": [
    "### step 4: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "173acaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gt(gt_path: str):\n",
    "    gt_3d = defaultdict(list)\n",
    "    gt_3d_range = defaultdict(list)\n",
    "    with open(gt_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip(\"\\n\").split()\n",
    "            start_f, end_f = int(line[0]), int(line[1])\n",
    "            instance_id = int(line[2])\n",
    "            x, y, z = float(line[3]), float(line[4]), float(line[5])\n",
    "            gt_3d[instance_id].append(np.array([x, y, z]))\n",
    "            gt_3d_range[instance_id].append([start_f, end_f])\n",
    "    return gt_3d, gt_3d_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eddecee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished evaluation!\n",
      "Threshold: 0.25 -- Precision: 0.233, Recall: 0.249\n",
      "Threshold: 0.50 -- Precision: 0.264, Recall: 0.281\n",
      "Threshold: 0.75 -- Precision: 0.331, Recall: 0.353\n",
      "Threshold: 1.00 -- Precision: 0.433, Recall: 0.463\n",
      "Threshold: 1.50 -- Precision: 0.594, Recall: 0.634\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baseline_root = os.path.join(os.getcwd(), \"results/baseline_results\")\n",
    "# predefined thresholds\n",
    "thresholds = [0.25, 0.5, 0.75, 1.0, 1.5]\n",
    "\n",
    "# load predictions\n",
    "num_gts_all = 0\n",
    "num_preds_all = 0\n",
    "tp_all, fp_all, fn_all = np.array([0] * len(thresholds)), np.array([0] * len(thresholds)), np.array([0] * len(thresholds))\n",
    "for seq_id, seq in enumerate(seq_list):\n",
    "    # per sequence precision and recall\n",
    "    cam_pose = json.load(open(os.path.join(data_root, \"raw_video_seqs\", seq, \"pv_pose.json\")))\n",
    "    all_frames = sorted(cam_pose.keys(), key=lambda x:int(x))\n",
    "\n",
    "    # load gt 3D points for the sequence\n",
    "    gt_path = os.path.join(data_root, \"annotations\", seq.replace(\"raw\", \"annotation\"), \"3d_center_annot.txt\")\n",
    "    gt_3d, gt_3d_range = load_gt(gt_path)\n",
    "\n",
    "    tp, fp, fn = np.array([0] * len(thresholds)), np.array([0] * len(thresholds)), np.array([0] * len(thresholds))\n",
    "    num_gts = 0\n",
    "    num_preds = 0\n",
    "    for f in all_frames:\n",
    "        # load baseline \n",
    "        baseline_f = os.path.join(baseline_root, seq, model_name, f+\".txt\")\n",
    "        baseline_res = {}\n",
    "        bsaeline_res_detScore = {}\n",
    "        with open(baseline_f, \"r\") as f1:\n",
    "            for l in f1:\n",
    "                l = l.rstrip(\"\\n\").split()\n",
    "                instance_id = int(l[0])\n",
    "                baseline_res[instance_id] = [np.array([float(l[1]), float(l[2]), float(l[3])])]\n",
    "                bsaeline_res_detScore[instance_id] = [float(l[4])]\n",
    "\n",
    "        num_preds += len(baseline_res.keys())\n",
    "        matched_ids = []\n",
    "        # compute distance to gt\n",
    "        for instance_id in gt_3d:\n",
    "            for i in range(len(gt_3d[instance_id])):\n",
    "                if int(f) >= gt_3d_range[instance_id][i][0] and int(f) <= gt_3d_range[instance_id][i][1]:\n",
    "                    num_gts += 1\n",
    "                    if instance_id not in baseline_res:\n",
    "                        fn += 1\n",
    "                        continue\n",
    "\n",
    "                    for t_idx, t in enumerate(thresholds):\n",
    "                        tp_flag = False\n",
    "                        assert len(baseline_res[instance_id]) == 1\n",
    "                        for j in range(len(baseline_res[instance_id])):\n",
    "                            dist = np.linalg.norm(gt_3d[instance_id][i] - baseline_res[instance_id][j])\n",
    "                            if dist <= t:\n",
    "                                tp[t_idx] += 1\n",
    "                                tp_flag = True\n",
    "                                # break\n",
    "                        if not tp_flag:\n",
    "                            fp[t_idx] += 1\n",
    "                            fn[t_idx] += 1\n",
    "\n",
    "                    matched_ids.append(instance_id)\n",
    "        fp += len(baseline_res.keys()) - len(matched_ids)\n",
    "\n",
    "    # compute precision and recall\n",
    "    precision = tp / num_preds\n",
    "    assert num_preds == (tp+fp)[0]\n",
    "    recall = tp / num_gts\n",
    "    tp_all += tp\n",
    "    fp_all += fp\n",
    "    fn_all += fn\n",
    "    num_gts_all += num_gts\n",
    "    num_preds_all += num_preds\n",
    "\n",
    "# overall Stats\n",
    "precision_all = tp_all / num_preds_all\n",
    "recall_all = tp_all / num_gts_all\n",
    "print(\"Finished evaluation!\")\n",
    "for t_idx, t in enumerate(thresholds):\n",
    "    print(\"Threshold: {:.2f} -- Precision: {:.3f}, Recall: {:.3f}\".format(t, precision_all[t_idx], recall_all[t_idx]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3250f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
